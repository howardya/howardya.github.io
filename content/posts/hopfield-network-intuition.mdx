---
title: Physical Intuition of Hopfield Network and Hebbian Learning
date: "2020-12-25"
description: "Abc"
categories: ["machine-learning-algorithm"]
tags: ["hopfield", "machine-learning"]
---

import Image from "../../src/components/Utils/image"

There are many good articles \cite{hopfieldIsAllYouNeed} and videos \cite{hopfieldYoutube} introducing the idea of Hopfield Network \cite{Hopfield2554} and they are pretty good. In this article however, my attempt is to describe the concept in a more intuitive manner, linking the concept to geometry and quantum physics.

## Objectives of Hopfield Network

The goal of Hopfield Network is to model how our memory might work in practise, inside our brain. We often have the following experience: We cannot recall exactly the thing we want to remember, it can be a song, a book, a date or a person's name. However, if we are prompted with partial memory of it, suddenly there is the eureka moment and the memory comes back.

This is exactly what Hopfield Network is aiming to model.

## Toy Example - Single Memory State

Suppose we only have one item to remember, for our Hopfield Network to learn. As usual, we will encode this item as a mathematical object, a vector to be precise. I shall denote its representation as a column vector $|x\rangle$. (I am using the Dirac notation here but it is nothing more than just a column vector.)

Our job is to start with an arbitrary vector, $|y_0\rangle$, and then update the vector in a step by step manner,$|y_n\rangle \rightarrow |y_{n+1}\rangle$. The update should ideally be done in a manner that, after many iterations, $|y_{\infty}\rangle \approx |x\rangle$ as close as possible.

To proceed, we first define what does it mean the closeness. We can define the projector $P_x = |x\rangle\langle x|$ (In the vector space notation, this is simply, $xx^T$). Closeness is then defined as

$$
\text{Closeness of }x\text{ vs }y = \langle y|P_x|y\rangle
$$

This is where physical intuition helps. There are a few ways to interpret the definition above:

1. Geometrical Interpretation: In vector notations, the closeness is essentially $y^Txx^Ty = (x\cdot y)^2 = \cos^2 \theta$, where $\theta$ is the angle between the vector $x$ and $y$.

2. Quantum Measurement Interpretation (Born's Rule): The above formula is simply the formula to calculate the probability of obtaining outcome $x$ if we perform the a measurement on $|y\rangle$. The formula can be simplified to obtain the usual expression of Born's rule, $\langle y|P_x|y\rangle=\langle y|x\rangle\langle x|y\rangle = \left| \langle y|x\rangle \right|^2$.

3. Operatonal Interpretation: The above formula is also the definition of quantum fidelity between two pure states and can be interpreted as the best probability that the state $|y\rangle$ can be pass any tests meant to distinguish it from $|x\rangle$.

In above, I have chosen to define using closeness and thus the update procedure is to increase it. One can also define it in terms of energy. Mathematically, any function that "invert the sign of closeness" can be treated as an "energy" function (more on this later). Physically, in the spin contexts, two spins along the same direction is a more stable configurations as opposed to opposite direction. Thus, a valid energy function would be (both mathematically and physically):

$$
\text{Energy}(y) = -\frac{1}{2}\langle y|x\rangle\langle x|y\rangle
$$

## Hebbian Updates - Single Memory State

Now that we have the closeness definition, the next key piece is to discover an algorithm to perform the update to the objects $|y_n\rangle$, in order to increase its closeness (equivalently, decrease its energy). It is good to point out here that Hopfield Network typically deals with discrete variables, in the sense that all the elements in the vectors $|x\rangle$ or $|y_n\rangle$ are binary, $\{-1,1\}$ for instance.

Suppose now we would like to update the vector $|y_n\rangle$: We first project it onto the subspace spanned by $|x\rangle$, and this can be done by applying the projector $P_x$ onto $|y_{n}\rangle$:

$$
P_x|y_{n}\rangle = |x\rangle\underbrace{\langle x|y_n\rangle}_{\mathrlap{\text{length of projection onto }|x\rangle}}
$$

We are going to allow ourselves to modify the elements in the vector $|y_n\rangle$ individually. To get a sense of how to do that, notice the following:

1. Our definition of goodness is invariant with respect to sign change: We are satisfied if we get something parallel to $|x\rangle$, i.e. either close to $|x\rangle$ or $-|x\rangle$.

2. If $|y_{n}\rangle$ is "closer" to $|x\rangle$, then we aim to update the vector so that $|y_{n+1}\rangle$ is further closer to $|x\rangle$. However, if $|y_{n}\rangle$ is "closer" to $-|x\rangle$, then we aim to update the vector such that $|y_{n+1}\rangle$ is further closer to $-|x\rangle$. Whether it is closer to $|x\rangle$ or $-|x\rangle$ is determined by the sign of $\langle x|y_{n}\rangle$, since this is the dot product, or cosine of the angle between two vectors.

3. Now, for the actual update, we randomly pick an element of $y_n^{(i)} \coloneqq (|y_n\rangle)_{i}$. Then we set $y_{n+1}^{(i)} \rightarrow x^{(i)}$ when $\langle x|y_{n}\rangle > 0$. Recall that if $\langle x|y_{n}\rangle > 0$, it means the angle between the two vectors are acute, so $|y_{n}\rangle$ is "closer" to $|x\rangle$, so we want the new vector's $i^{th}$ element to be equal to $|x\rangle$. On the contrary, if $\langle x|y_{n}\rangle < 0$, we set $y_{n+1}^{(i)} \rightarrow -x^{(i)}$. In summary:

$$
y_{n+1}^{(i)} = \left\{\begin{array}{ll} x^{(i)} & \text{if }\langle x|y_{n}\rangle > 0 \\ -x^{(i)} & \text{if }\langle x|y_{n}\rangle < 0 \end{array}\right.
$$

Since the elements of the vector $|x\rangle$ are $\pm1$, the previous equation can be further simplified and compactly summarized as $y_{n+1}^{(i)} = \text{sign}\left( x^{(i)} \langle x|y_{n}\rangle \right)$. This is equivalent to the Hebbian Learning update rule that is presented in major literatures:

$$
|y_{n+1}\rangle = \text{sign}\left( |x\rangle \langle x|y_{n}\rangle \right) = \text{sign}\left( P_x|y_{n}\rangle \right)
$$

## Extensions of Hopfield vs Foundations of Quantum Physics

To understand how classical RAM works, it is instructive to consider the following cat example:

Suppose you have 8 cute cats, and you keep a note of which cat has been fed, and which is still hungry (poor cat). We shall use the notation as followed: $0$ indicates that the cat is hungry and $1$ indicates that the cat has been fed. For example, in the Figure 1 below, we can see which cats are have been fed (Cat 1/3/4/5/7) and which cats are hungry (Cat 2/6/8).

<Image
	fileName="quantum-ram/figure-1-cat-data.png"
	className="medium-image"
	alt="Figure 1: Cats with indicators whether they have been fed already or still hungry."
/>

They are stored in RAM as binary data: RAM will assign an address to the location it stores the data

| Cats Index | Status | RAM Data | RAM Address |
| :--------: | :----: | :------: | :---------: |
|   Cat 1    |  Full  |    1     |    0x000    |
|   Cat 2    | Hungry |    0     |    0x001    |
|   Cat 3    |  Full  |    1     |    0x010    |
|   Cat 4    |  Full  |    1     |    0x011    |
|   Cat 5    |  Full  |    1     |    0x100    |
|   Cat 6    | Hungry |    0     |    0x101    |
|   Cat 7    |  Full  |    1     |    0x110    |
|   Cat 8    | Hungry |    0     |    0x111    |

Notice that I have also included the address pointing to the location where the RAM data of each cat is being stored.

## Classical RAM Retrieval

1. **Superposition** - In the classical RAM, there is always a unique address that we are interested in. Sure, we may want to find out what are the data stored in address 1, address 2, ... and so on, but the we will never _mix_ up query address 1 and address 2 together. In quantum computers, it is necessary to be able query a **superposition** of the quantum state in address 1 with the quantum state of address 2. For instance, we want the state $ \frac{1}{\sqrt{2}} \left( |0x001\rangle + |0x110\rangle  \right) $. There is no classical analogue to this.

2. **Decoherence** - Classical RAM can in principle store the data as long as its power is on. Quantum computers do not have such luxury because quantum systems are extremely delicate. Quantum memory will decohere (destroyed into unusable state) in a short time span. How short is the time scale we are talking? Miliseconds \cite{Miao1493}.

3. **Entanglement** - Entanglement is a double edged sword. It is a useful feature but it is also a pain to deal with. In the Figure 2 above, you may notice that the second bit of the address controls _all_ the gates at the second level. The number of gates at the $n^{th}$ level scales exponentially. In a quantum computer, this would mean that we need to interact exponentially many quantum objects and such interaction will create a massive entanglement between all the objects. This kind of system is extremely prone to decoherence.

## Quantum RAM

Despite the problems addressed above, there are advances in terms of algorithms and implementations. Here I illustrate the algorithm called **bucket-brigade** quantum RAM \cite{qramPRL,qramPRA}.

To simplify the notations, I shall reduce the levels to only two levels. Thus the addresses can only be one of the following ${0x00,0x01,0x10,0x11}$. Inside the quantum RAM, holds the information, which in this quantum states. The following table provides a summary.

|              Address              |                 States                 |
| :-------------------------------: | :------------------------------------: |
| $\left\vert0x00\right\rangle_{A}$ | $\left\vert Cat_{00}\right\rangle_{M}$ |
| $\left\vert0x01\right\rangle_{A}$ | $\left\vert Cat_{01}\right\rangle_{M}$ |
| $\left\vert0x10\right\rangle_{A}$ | $\left\vert Cat_{10}\right\rangle_{M}$ |
| $\left\vert0x11\right\rangle_{A}$ | $\left\vert Cat_{11}\right\rangle_{M}$ |

The subscripts $A$ and $M$ here are to distinguish the systems from address vs memory states.

However, remember that we have a quantum computer: thus, any _linear combination_ of the addresses are also valid. For example, a user can specify the following valid address:

$$
\alpha*{00} \left|0x00\right\rangle*{A} + \alpha*{11} \left|0x11\right\rangle*{A}
$$

where $\left|\alpha_{00}\right|^2 + |\alpha_{11}\vert^2 = 1$. The quantum computer, together with quantum RAM, should return the user the following states:

$$
\alpha*{00}
\left|0x00\right\rangle*{A}
\left|Cat*{00}\right\rangle*{M} +
\alpha*{11}
\left|0x11\right\rangle*{A}
\left|Cat*{11}
\right\rangle*{M}
$$

## Quantum RAM Retrieval Process

To understand the idea behind the retrieval from a quantum RAM, the authors proposed that we first prepare our quantum RAM in the following states as shown in the figure below. The circles refer to a switch is initially in the off state.

<Image
	fileName="quantum-ram/figure-3-BB-step1.png"
	className="small-image"
	alt="Figure 3: Initialization of quantum RAM."
/>

We then sequentially send the quantum address from _top to bottom_, one qubit by one qubit. To understand the process, it is good to start with simple cases.

### A. Classical address in quantum RAM

### B. Quantum address in quantum RAM

## Final Remarks

@@bibliography@@
@article {Hopfield2554,
author = {Hopfield, J J},
title = {Neural networks and physical systems with emergent collective computational abilities},
volume = {79},
number = {8},
pages = {2554--2558},
year = {1982},
doi = {10.1073/pnas.79.8.2554},
publisher = {National Academy of Sciences},
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences}
},
@manual{hopfieldIsAllYouNeed,
title = {Hopfield Networks is All You Need},
author = {Hubert Ramsauer and Bernhard Schafl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Milena Pavlović and Geir Kjetil Sandve and Victor Greiff and David Kreil and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
year = {2020},
url={https:\/\/ml-jku.github.io\/hopfield-layers\/}
}
@manual{hopfieldYoutube,
title = {ANN as Mathematical Machine},
author = {Math4IQB},
year = {2020},
url={http:\/\/youtube.com\/watch?v=gfPUWwBkXZY}
}
@@bibliography@@

$$
$$
