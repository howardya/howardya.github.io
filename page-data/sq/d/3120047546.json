{"data":{"allMdx":{"edges":[{"node":{"frontmatter":{"date":"2021-01-10","description":"Explaining the difference between FT, DFT and FFT.","title":"Fourier Transform - FT vs DFT vs FFT"},"slug":"posts/fast-discrete-fourier-transform","rawBody":"---\ntitle: Fourier Transform - FT vs DFT vs FFT\ndate: \"2021-01-10\"\ndescription: \"Explaining the difference between FT, DFT and FFT.\"\ncategories: [\"\"]\ntags: [\"fourier-transform\", \"fft\"]\n---\nimport Image from \"../../src/components/Utils/image\"\n\nimport diracDeltaExponentialFile from '../../src/pdf/DiracDeltaExponential.pdf' \n\n\nFourier transform (FT) is perhaps one of the most important mathematical tool in the modern digital world today. It is used by digital devices to decode and encode digital signals whenever we stream Youtube, listen to music, make a call. (Un)surprisingly, it is also the core mathematical tools in quantum physics, giving rise to the infamous <a href=\"https://en.wikipedia.org/wiki/Uncertainty_principle\" target=\"_blank\">Heisenberg Uncertainty Principle</a>.\n\nThere are a few variants or implementations of Fourier transform such as discrete Fourier transform and fast Fourier transform. Their differences are subtle and can be quite confusing.\n\nIn this article, I would like to give an introduction and clarification to the differences between the them, in particular:\n1. Fourier Transform (FT)\n2. Discrete-Time Fourier Transform (DTFT)\n3. Discrete Fourier Transform (DFT)\n4. Fast Fourier Transform (FFT)\n\n## Fourier Transform (FT)\n\nFourier transform (FT) is the theoretical framework behind *all variants* of Fourier transform, it is thus essential to understand it in order to understand it first. \n\nFT can be understood in multiple ways: first of all, it is a convenient and useful tool to analyze *any* continuous function (not necessarily signals in the engineering context) from a different perspectives. When you think of music, you may think of them as (vibrations) amplitude across the time domain, or you may think of it as frequency distributions. They are the same but offer deeper insights: two sides of the same coin. In fact, the wave-particle duality in quantum physics is a consequence of such performing measures in space that is dual to one another.\n<Image\n\tfileName=\"fourier-transform/figure-1-analogy-fourier-transform.png\"\n\tclassName=\"small-image\"\n\talt=\"Figure 1: Analogy of Fourier Transform: Analyzing light by looking at its components.\"\n/>\n\n A sine wave for example, displayed as a sinusoidal change in the aplitude in time domain, is nonetheless a very well defined function in the frequeny domain: it has a fixed frequency and thus displays as a fixed deterministic point in the frequency domain distribution. Fourier transform aims to connect the amplitude distributions in these two spaces.\n\nThe mathematical definition of Fourier transform, is given as followed:\n$$\n\\color{blue}{\\mathcal{F}(p) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty dx \\; f(x) e^{-ipx}}\n$$\nwhere the original function $f(x)$ on the space of $x$ has been transformed into its dual space, $p$ and the distribution on the dual space is given by $\\mathcal{F}(p)$. \n\nThe physical meaning of $x$ and $p$ depends on the context. They can be the position-momentum (physics) or time-frequency (engineering). The transform can be reversed through the inverse Fourier transform defined by:\n$$\n\\color{blue}{f(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty dp \\; \\mathcal{F}(p) e^{ipx}}\n$$\n\n## Discrete-Time Fourier Transform (DTFT)\n\nOnce we are comfortable with FT, the next natural idea is DTFT, as opposed to DFT. Discrete time Fourier transform, as the name suggests, caters to signals which are discretely recorded in the time domain, at fixed intervals, mimicking how real world signals would have been measured and taken. However, even though the inputs to DTFT are discrete, the outputs in the frequency domain are continuous - thus the *discrete-time* name. \n\nTo illustrate the disreteness in the time domain, consider the original function $f(x)$ which is sampled discretely, at a fixed interval $\\Delta_x$. This can be represented using Dirac's distribution as\n$$\nf^{DT}(x) = \\sum_{n=-\\infty}^{\\infty} f(x) \\delta(x-n\\Delta_x)\n$$\n<Image\n\tfileName=\"fourier-transform/figure-2-discrete-time-fourier-transform.png\"\n\tclassName=\"medium-image\"\n\talt=\"Figure 2: Discrete time signals, at fixed interval of time, indefinitely.\"\n/>\n\n\nOnce we have the representation of the above signals, we can then apply the standard Fourier transform to the above discrete version of $f(x)$ to obtain\n$$\n\\mathcal{F}^{DTFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty dx \\; f^{DT}(x) e^{-ipx}\n$$\n$$\n\\mathcal{F}^{DTFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty dx \\; \\sum_{n=-\\infty}^{\\infty} f(x) \\delta(x-n\\Delta_x) e^{-ipx}\n$$\n$$\n\\color{blue}{\\mathcal{F}^{DTFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\sum_{n=-\\infty}^{\\infty} f(n\\Delta_x) e^{-ipn\\Delta_x}}\n$$\nThe last equation is then the definition of DTFT. Note that it is a continuous function of $p$, even though the inputs are discrete.\n\nOne interesting thing to note is that if $\\Delta_x = \\Delta'$, that is it is independent of $x$, then the DTFT is periodic by nature. This can be seen easily by noticing that\n$$\n\\mathcal{F}^{DTFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\sum_{n=-\\infty}^{\\infty} f(n\\Delta') e^{-ipn\\Delta'} = \\frac{1}{\\sqrt{2\\pi}} \\sum_{n=-\\infty}^{\\infty} f(n\\Delta') e^{-in\\Delta'(p + \\frac{2\\pi}{\\Delta'} k)} = \\mathcal{F}^{DTFT}(p + \\frac{2\\pi}{\\Delta'} k)\n$$\nwhere $k$ is any integers. The periodicity is thus $\\frac{2\\pi}{\\Delta'}$.\n\nThe intuition is that in DTFT, we have discarded a large chunks of information in our original time domain function, and in the frequency domain, it means that we only need to specify the frequency amplitude for one period. The length of the period depends on the gap $\\Delta'$.\n\n> The bigger the gap $\\Delta'$ is, the more information we are ignoring in the time domain. Thus we need to specify smaller information in the frequency domain, and this is reflected by the smaller period $\\frac{2\\pi}{\\Delta'}$ in the frequency domain.\n\nThe inverse of DTFT is given by \n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty dp \\; \\mathcal{F}^{DTFT}(p) e^{ipx}\n$$\nDue to the periodicity of $\\mathcal{F}^{DTFT}(p)$, it can also be written equivalently as\n$$\n\\color{blue}{f(n\\Delta') = \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\frac{2\\pi}{\\Delta'}} dp \\; \\mathcal{F}^{DTFT}(p) e^{ipn\\Delta'}}\n$$\n\n## Discrete Fourier Transform\n\nIn the previous section, the DTFT of a discrete version is a continuous function in the frequency domain. While it is good to capture the continuous behaviour in the frequency domain, it is not practical for computer to deal with continuous function.\n\nDiscrete Fourier transform (DFT) is another variant such that it takes in finite discrete inputs in the time domain and output finite discrete outputs in the frequency domain, so that computers can work efficiently on it.\n\nThree changes we are going to make:\n1. Assume that the sampling is only done at finite number of $N$ points, corresponding to $\\Delta, 2\\Delta,\\ldots,N\\Delta$. Thus we have $f^{D}(x) = \\sum_{n=1}^{N} f(x) \\delta(x-n\\Delta)$.\n2. Assume that the sampled data in the time domain is periodic outside the range from $\\Delta$ to $N\\Delta$. Thus $f^{D}(x + N\\Delta) = f^{D}(x)$. With this assumption, we can extend the function to infinite sums again, $f^{D}(x) = \\sum_{n=-\\infty}^{\\infty} f(x) \\delta(x-n\\Delta)$. We only need to note that $f(x+N\\Delta) = f(x)$.\n3. Recall that discreteness in the time domain implies that the frequency domain function is periodic. Similar, a periodic time domain function will imply discreteness in the frequency domain.\n4. Periodic and discreteness in the frequency domain means that we need only to keep finite number of terms.\n\nLet us derive how Fourier transform of the function $f^{D}(x)$ looks like.\n$$\n\\mathcal{F}^{DFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty dx \\; f^{D}(x) e^{-ipx}\n$$\n$$\n\\mathcal{F}^{DFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty dx \\; \\sum_{n=-\\infty}^{\\infty} f(x) \\delta(x-n\\Delta) e^{-ipx}\n$$\n$$\n\\mathcal{F}^{DFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\sum_{n=-\\infty}^{\\infty} f(n\\Delta) e^{-ipn\\Delta}\n$$\n\nIn the last equations, we can replace the single summation with a double summations\n$$\n\\sum_{n=-\\infty}^{\\infty} F(n) \\rightarrow \\sum_{n=1}^{N} \\sum_{m=-\\infty}^\\infty F(n+mN)\n$$\nThus, we have\n$$\n\\mathcal{F}^{DFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\sum_{n=1}^{N} \\sum_{m=-\\infty}^\\infty f(n\\Delta + mN\\Delta) e^{-ipn\\Delta}e^{-ipmN\\Delta}\n$$\nHowever, we have $f(n\\Delta + mN\\Delta) = f(n\\Delta)$, due to the periodicity as described in the second point above.\n$$\n\\mathcal{F}^{DFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\sum_{n=1}^{N} f(n\\Delta)e^{-ipn\\Delta} \\sum_{m=-\\infty}^\\infty  e^{-ipmN\\Delta}\n$$\n$$\n\\mathcal{F}^{DFT}(p) = \\frac{1}{\\sqrt{2\\pi}} \\sum_{n=1}^{N} f(n\\Delta)e^{-ipn\\Delta} \\frac{2\\pi}{N\\Delta} \\sum_{l=-\\infty}^{\\infty} \\delta(p-l \\frac{2\\pi}{N\\Delta})\n$$\nThe second equation above is due to the identity below, which can be found at <a target=\"_blank\" href=\"http://fourier.eng.hmc.edu/e102/lectures/ExponentialDelta.pdf\">here</a>. (The pdf can also be downloaded <a href={diracDeltaExponentialFile} download>here</a>)\n$$\n\\frac{1}{F} \\sum_{m=-\\infty}^{\\infty} e^{-i2\\pi mf/F} = \\sum_{l=-\\infty}^{\\infty} \\delta(f-lF)\n$$\n\nTo simplify the equation further, notice that the Fourier transform $\\mathcal{F}^{DFT}(p)$ is periodic with period $\\frac{2\\pi}{\\Delta}$. Thus we only need to know the values for $\\mathcal{F}^{DFT}(p)$ from, say, $0$ to $\\frac{2\\pi}{\\Delta}$. Combined this with the fact we have a discrete Dirac delta functions, it is obvious that we only need the $N$ values $\\mathcal{F}^{DFT}(p)$ where $p=\\frac{2\\pi}{N\\Delta}, 2\\frac{2\\pi}{N\\Delta},\\ldots,N\\frac{2\\pi}{N\\Delta}$. These discrete values are given by\n$$\n\\color{blue}{\\mathcal{F}^{DFT}(k\\frac{2\\pi}{N\\Delta}) = \\frac{1}{\\sqrt{2\\pi}} \\frac{2\\pi}{N\\Delta} \\sum_{n=1}^{N} f(n\\Delta)e^{-i k2\\pi\\frac{n}{N}}, \\;\\;\\; k=1,2,\\ldots,N}\n$$\n\nThe inverse of DFT can be derived equivalently by the same logic as above. However, a simpler equivalent relationship can be observed as followed. This is the Fourier transform identity we have painstakingly derived abovee:\n$$\nf^{D}(x) = \\sum_{n=-\\infty}^{\\infty} f(x) \\delta(x-n\\Delta) \\;\\;\\; \\xrightarrow{DFT} \\;\\;\\; \\mathcal{F}^{DFT}(k\\frac{2\\pi}{N\\Delta}) = \\frac{1}{\\sqrt{2\\pi}} \\frac{2\\pi}{N\\Delta} \\sum_{n=1}^{N} f(n\\Delta)e^{-i k2\\pi\\frac{n}{N}}\n$$\n\nIf we now make the following mapping: $x$ changed to $p$, $\\Delta$ changed to $\\frac{2\\pi}{N\\Delta}$, $i$ changed to $-i$, then immediately can see that following inverse DFT:\n$$\nF^{DFT}(p) = \\sum_{n=-\\infty}^{\\infty} f(p) \\delta(p-n\\frac{2\\pi}{N\\Delta}) \\;\\;\\; \\xrightarrow{\\text{inverse}\\; DFT}     \\;\\;\\; f(k\\Delta) = \\frac{1}{\\sqrt{2\\pi}} \\Delta \\sum_{n=1}^{N} F^{DFT}(n\\frac{2\\pi}{N\\Delta})e^{i k2\\pi\\frac{n}{N}}\n$$\n\nNote that in some literatures, you may see the notation $f(n\\Delta) \\rightarrow x_n$ used. Furthermore, the coefficients in front may be missing. However, they are consistent. There is some arbitrariness in terms of how we define Fourier transform. The utmost important thing is that after one round of Fourier transform and followed by inverse Fourier transform, we retrieve the original data. It does not matter so much the whether we put all the coefficients in the first step or the second step.\n\n\n\n\n## Fast Fourier Transform\n\nAlright, phew, that was an intense exercise. How about FFT then? Conceptually, FFT is similar to DFT. The real difference is with the way it is being calculated. In the formulation above, note that we have $N$ discrete values. The output of DFT is also a set of $N$ values in the frequency domain. However, to calculate each of the $N$ value in the frequency domain, we need to perform multiplication $N$ times. Thus, in total, we require $\\mathcal{O}(N^2)$ computations. \n\nFFT aims to speed up these computations, by reducing the $N^2$ calculations into $\\mathcal{O}(n\\log n)$, a dramatic increase in the computation speed! \n<Image\n\tfileName=\"fourier-transform/figure-3-fft-speed-up.png\"\n\tclassName=\"medium-image\"\n\talt=\"Figure 3: Speed up in computations of FFT vs DFT.\"\n/>\n\n\nMost of us likely will not need to know the exact algorithm to use FFT.\n\n## Summary\n\nTable below summarizes the comparisons between FT, DTFT, DFT and FFT. \n\n|  | FT | DTFT | DFT | FFT |\n|:--------:| :----: | :------: | :---------: | :---------: |\n|Input Type<br/>(Time Domain)|Continuous|Discrete|Discrete|Discrete|\n|Output Type<br/>(Frequency Domain)|Continuous|Continuous|Discrete|Discrete|\n|Periodicity<br/>(Time Domain)|None|None|Yes<br/>Assumed repeats itself|Yes<br/>Assumed repeats itself|\n|Periodicity<br/>(Frequency Domain)|None|Yes|Yes|Yes|\n|Computation Type|Analytical Integration|Analytical Integration|Discrete Sum|Discrete Sum|\n\n<br /><br />\n\n<br /><br />\n\n### References\n\n@@bibliography@@\n@@bibliography@@"}},{"node":{"frontmatter":{"date":"2020-12-29","description":"Introducing the concept of Quantum version of Hopfield Network.","title":"Quantum Hopfield Network"},"slug":"posts/quantum-hopfield-network","rawBody":"---\ntitle: Quantum Hopfield Network\ndate: \"2020-12-29\"\ndescription: \"Introducing the concept of Quantum version of Hopfield Network.\"\ncategories: [\"quantum-machine-learning-algorithm\"]\ntags: [\"hopfield\", \"machine-learning\", \"quantum\"]\n---\n\nimport Image from \"../../src/components/Utils/image\"\n\nA few different extensions have been proposed to implement quantum version of Hopfield Network \\cite{Rotondo_2018,Seddiqi_2014,Rebentrost_2018} using different mechanisms to encode the training data as well as different implementations to the retrieve *content addressable memory*.\n\nIn this article, I will illustrate quantum Hopfield Network using the implementation discussed in \\cite{Rebentrost_2018}.\n\nI will assume that you have understood how a classical Hopfield Network works, and if not you may want to go through this <a href=\"hopfield-network-intuition\">article</a> first.\n\n## Encoding of Data\n\nRecall that Hopfield Network intends to encode training data or activation pattern, and that requires a representation or encoding in our computer. In the classical computer, these would be standard bits. In quantum computer they can be represented as the phases in a quantum system. \n\nFor example, consider the $m^{th}$ training data which is a $d$-dimensional bits, $(x^{(m)}_1,x^{(m)}_2,\\ldots,x^{(m)}_d)$, then it is represented in quantum computer as\n$$\n|x^{(m)}\\rangle = \\sum_{i=1}^d x_i^{(m)} |i\\rangle\n$$\n\nwhere the $|i\\rangle$ are the standard basis. This can be done using <a href=\"quantum-ram\">quantum RAM</a> \\cite{qramPRL}.\n\n## Quantum Hebbian Learning\n\nIn the <a href=\"hopfield-network-intuition\">classical setting</a>, Hebbian Learning means the construction/learning of a matrix which can then be used for retrieving content addressable memory. The matrix is constructed as \n$$\nM = \\frac{1}{M} \\sum_m x_m x_m^T\n$$\n\nIn quantum computer however, such matrix can be represented as a density matrix, $\\rho$ defined by\n$$\n\\rho = \\frac{1}{M} \\sum_m |x^{(m)}\\rangle \\langle x^{(m)}|\n$$\n\nThe density matrix can be constructed either through qRAM \\cite{qramPRL} or efficient state preparation \\cite{efficientStatePreparation}.\n\n## Updating of Quantum States\n\nIn the classical setting, we update the vector iteratively using the rule as described in <a href=\"hopfield-network-intuition\">here</a>:\n$$\n|y_{n+1}\\rangle = \\text{sign}\\left( |x\\rangle \\langle x|y_{n}\\rangle \\right) = \\text{sign}\\left( P_x|y_{n}\\rangle \\right)\n$$\n\nIn the quantum version, any changes to a state must be done through unitary operators and any output must be done through measurements. \n\nIn order to efficiently map the operations to quantum states, the authors in \\cite{Rebentrost_2018} first reformulated an equivalent updating rule, using matrix inversion. The rationale for using matrix inversion is because it is one of the operations quantum physicists know how to perform it well \\cite{Harrow_2009}. \n\nThe key observation is to view the update as a pure optimization task minimizing the enery subject to the constraint that the optimized vector equal to the partial input pattern we are interested to recall. In other words, in the classical Hopfield Network, equivalent formulation is to minimize the following Lagrangian\n$$\n\\mathcal{L} = - \\frac{1}{2} \\boldsymbol{x^T}W\\boldsymbol{x} + \\boldsymbol{\\theta^T}\\boldsymbol{x} - \\boldsymbol{\\lambda^T}\\left( P\\boldsymbol{x} - \\boldsymbol{x^\\text{input}}  \\right) + \\frac{\\gamma}{2} \\boldsymbol{x^T}\\boldsymbol{x}\n$$\nwhere $\\boldsymbol{\\lambda}$ is the lagrange multiplier, $\\gamma$ is the penalty function multiplier and $P$ is the projector onto the subspace spanned by $\\boldsymbol{x^{\\text{input}}}$. The solution to the above opmization, $\\boldsymbol{x^*}$ is given by\n$$\n\\left( \\begin{array}{c} \\boldsymbol{x} \\\\ \\boldsymbol{\\lambda} \\end{array} \\right) = A^{-1} |w\\rangle \\\\\n\\hspace{1mm}\\\\\nA \\equiv \\left( \\begin{array}{cc} W-\\gamma\\mathcal{I}_d & P \\\\ P & 0 \\end{array} \\right), \\hspace{5mm} |w\\rangle \\equiv \\left( \\begin{array}{c} \\boldsymbol{\\theta} \\\\ \\boldsymbol{x^\\text{input}} \\end{array} \\right) \\\\\n$$\n\nThe state $A^{-1}|w\\rangle$ can be prepared rather efficiently. For specific details, you may refer to \\cite{Rebentrost_2018}.\n\nOnce the final state is prepared, it can then be used to extract the relevant information, be it measurements or tomography.\n\n## Final Remark\n\nIn this article, I have shown how the classical Hopfield Network is extended to quantum Hopfield Network, at least in principle. The key ingredient for this to be possible is the ability for quantum computer to perform matrix inversion in an efficient manner. This operation is one of the key hallmark of quantum computing and quantum algorithms.\n\n\n<br /><br />\n\n<br /><br />\n\n### References\n\n@@bibliography@@\n@@bibliography@@\n"}},{"node":{"frontmatter":{"date":"2020-12-25","description":"Explaining the intuition behind Hopfield Network and Hebbian Learning, with links to and insights from Quantum Physics.","title":"A Physically Intuitive Introduction To Hopfield Network and Hebbian Learning"},"slug":"posts/hopfield-network-intuition","rawBody":"---\ntitle: A Physically Intuitive Introduction To Hopfield Network and Hebbian Learning\ndate: \"2020-12-25\"\ndescription: \"Explaining the intuition behind Hopfield Network and Hebbian Learning, with links to and insights from Quantum Physics.\"\ncategories: [\"machine-learning-algorithm\"]\ntags: [\"hopfield\", \"machine-learning\"]\n---\n\nimport Image from \"../../src/components/Utils/image\"\n\n*(This article is about classical, non-quantum Hopfield Network and its intuition, even though notations used mimic those used in quantum physics. Check <a href=\"quantum-hopfield-network\">here</a> for quantum Hopfield network.)*\n\nThere are many good articles \\cite{hopfieldIsAllYouNeed} and videos \\cite{hopfieldYoutube,hopfieldYoutube2019} introducing the idea of Hopfield Network \\cite{Hopfield2554} and they are pretty good. In this article however, my attempt is to describe the concept in a more intuitive manner, linking the concept to geometry and quantum physics.\n\n## Objectives of Hopfield Network\n\nThe goal of Hopfield Network is to model how our memory might work in practise, inside our brain. We often have the following experience: We cannot recall exactly the thing we want to remember, it can be a song, a book, a date or a person's name. However, if we are prompted with partial memory of it, suddenly there is the eureka moment and the memory comes back.\n\nThis is exactly what Hopfield Network is aiming to model.\n\n## Toy Example - Single Memory State\n\nSuppose we only have one item to remember, for our Hopfield Network to learn. As usual, we will encode this item as a mathematical object, a vector to be precise. I shall denote its representation as a column vector $|x\\rangle$. (I am using the Dirac notation here but it is nothing more than just a column vector.)\n\nOur job is to start with an arbitrary vector, $|y_0\\rangle$, and then update the vector in a step by step manner,$|y_n\\rangle \\rightarrow |y_{n+1}\\rangle$. The update should ideally be done in a manner that, after many iterations, $|y_{\\infty}\\rangle \\approx |x\\rangle$ as close as possible.\n\nTo proceed, we first define what does it mean the closeness. We can define the projector $P_x = |x\\rangle\\langle x|$ (In the vector space notation, this is simply, $xx^T$). Closeness is then defined as\n\n$$\n\\text{Closeness of }x\\text{ vs }y = \\langle y|P_x|y\\rangle\n$$\n\nThis is where physical intuition helps. There are a few ways to interpret the definition above:\n\n1. Geometrical Interpretation: In vector notations, the closeness is essentially $y^Txx^Ty = (x\\cdot y)^2 = \\cos^2 \\theta$, where $\\theta$ is the angle between the vector $x$ and $y$.\n\n2. Quantum Measurement Interpretation (Born's Rule): The above formula is simply the formula to calculate the probability of obtaining outcome $x$ if we perform the a measurement on $|y\\rangle$. The formula can be simplified to obtain the usual expression of Born's rule, $\\langle y|P_x|y\\rangle=\\langle y|x\\rangle\\langle x|y\\rangle = \\left| \\langle y|x\\rangle \\right|^2$.\n\n3. Operatonal Interpretation: The above formula is also the definition of quantum fidelity between two pure states and can be interpreted as the best probability that the state $|y\\rangle$ can be pass any tests meant to distinguish it from $|x\\rangle$.\n\nIn above, I have chosen to define using closeness and thus the update procedure is to increase it. One can also define it in terms of energy. Mathematically, any function that \"invert the sign of closeness\" can be treated as an \"energy\" function (more on this later). Physically, in the spin contexts, two spins along the same direction is a more stable configurations as opposed to opposite direction. Thus, a valid energy function would be (both mathematically and physically):\n\n$$\n\\text{Energy}(y) = -\\frac{1}{2}\\langle y|x\\rangle\\langle x|y\\rangle\n$$\n\n## Hebbian Updates - Single Memory State\n\nNow that we have the closeness definition, the next key piece is to discover an algorithm to perform the update to the objects $|y_n\\rangle$, in order to increase its closeness (equivalently, decrease its energy). It is good to point out here that Hopfield Network typically deals with discrete variables, in the sense that all the elements in the vectors $|x\\rangle$ or $|y_n\\rangle$ are binary, $\\{-1,1\\}$ for instance.\n\nSuppose now we would like to update the vector $|y_n\\rangle$: We first project it onto the subspace spanned by $|x\\rangle$, and this can be done by applying the projector $P_x$ onto $|y_{n}\\rangle$:\n\n$$\nP_x|y_{n}\\rangle = |x\\rangle\\underbrace{\\langle x|y_n\\rangle}_{\\mathrlap{\\text{length of projection onto }|x\\rangle}}\n$$\n\nWe are going to allow ourselves to modify the elements in the vector $|y_n\\rangle$ individually. To get a sense of how to do that, notice the following:\n\n1. Our definition of goodness is invariant with respect to sign change: We are satisfied if we get something parallel to $|x\\rangle$, i.e. either close to $|x\\rangle$ or $-|x\\rangle$.\n\n2. If $|y_{n}\\rangle$ is \"closer\" to $|x\\rangle$, then we aim to update the vector so that $|y_{n+1}\\rangle$ is further closer to $|x\\rangle$. However, if $|y_{n}\\rangle$ is \"closer\" to $-|x\\rangle$, then we aim to update the vector such that $|y_{n+1}\\rangle$ is further closer to $-|x\\rangle$. Whether it is closer to $|x\\rangle$ or $-|x\\rangle$ is determined by the sign of $\\langle x|y_{n}\\rangle$, since this is the dot product, or cosine of the angle between two vectors.\n\n3. Now, for the actual update, we randomly pick an element of $y_n^{(i)} \\coloneqq (|y_n\\rangle)_{i}$. Then we set $y_{n+1}^{(i)} \\rightarrow x^{(i)}$ when $\\langle x|y_{n}\\rangle > 0$. Recall that if $\\langle x|y_{n}\\rangle > 0$, it means the angle between the two vectors are acute, so $|y_{n}\\rangle$ is \"closer\" to $|x\\rangle$, so we want the new vector's $i^{th}$ element to be equal to $|x\\rangle$. On the contrary, if $\\langle x|y_{n}\\rangle < 0$, we set $y_{n+1}^{(i)} \\rightarrow -x^{(i)}$. In summary:\n\n$$\ny_{n+1}^{(i)} = \\left\\{\\begin{array}{ll} x^{(i)} & \\text{if }\\langle x|y_{n}\\rangle > 0 \\\\ -x^{(i)} & \\text{if }\\langle x|y_{n}\\rangle < 0 \\end{array}\\right.\n$$\n\nSince the elements of the vector $|x\\rangle$ are $\\pm1$, the previous equation can be further simplified and compactly summarized as $y_{n+1}^{(i)} = \\text{sign}\\left( x^{(i)} \\langle x|y_{n}\\rangle \\right)$. This is equivalent to the Hebbian Learning update rule that is presented in major literatures:\n\n$$\n|y_{n+1}\\rangle = \\text{sign}\\left( |x\\rangle \\langle x|y_{n}\\rangle \\right) = \\text{sign}\\left( P_x|y_{n}\\rangle \\right)\n$$\n\n> Hebbian Learning in Hopfield Network is to construct a projector for the pattern to memorize. To recall the pattern, we use the projector to project any starting pattern and tweak the bits of vector, repeatedly, such that eventually, the new pattern closely resembles the memory.\n\n## Biases\n\nIn the standard definition of Hopfield Network's energy, biases are added so that we no longer treat $1$ and $-1$ as equivalent. The update rule is only marginally more complicated but the intuition is the same. As such, we shall ignore biases in this article.\n\n## Multiple Memory states\n\nFrom above, we see that we associate a projector $P_x = |x\\rangle\\langle x|$ for the single memory vector $x$ we are trying to memorize inside a Hopfield Network. What happens if we have multiple states to remember? The straightforward way to do that is to define the following matrix\n\n$$\nM = \\frac{1}{N} \\sum_{x=1}^N P_x\n$$\n\nwhere $P_x$ is the projector associated to the pattern $x$ we would like our Hopfield Network to memorize. Note that $M$ is no longer a projector because the vectors $|x\\rangle$ may not be orthogonal with one another.\n\nThe closeness definition still stays the same but has slightly different physical interpretation:\n\n$$\n\\text{Closeness} = \\langle y_{n}|M|y_{n}\\rangle = \\frac{1}{N} \\sum_x \\left| \\langle y_{n}|x\\rangle \\right|^2\n$$\n\nIt is thus the weighted averaged probability to each of the memory stored in the network. By maximizing closeness, we aim to find the state $|y_{\\infty}\\rangle$ that maximizes this weighted probability to all the states in our memory.\n\nLet's see what happens if we apply the same Hebbian Learning algorithm as above,\n\n$$\ny_{n+1}^{(i)} = \\text{sign} \\left( \\frac{1}{N} \\sum_x x^{(i)} \\langle x|y_{n}\\rangle   \\right)\n$$\n\nThus, the decision to flip the $i^{th}$ bit carries through from earlier intuition, albeit the complicated weighting scheme, which is necessary due to multiple memory states.\n\n## Extensions of Hopfield vs Foundations of Quantum Physics\n\nThe above are what many called the *classical* Hopfield Network, and extensions have been proposed \\cite{krotov,Demircigil_2017}. I would refer readers to \\cite{hopfieldIsAllYouNeed} for a wonderful review of the matters. Here I take a slightly different approach and show its connection to the foundations of quantum physics.\n\nRecall that in above, both the single and multiple memory states, there is the _common quantity_ in the formula closeness (or energy) definition: the probability/Born's rule/squared-dot-product $|\\langle x|y_{n}\\rangle|^2$. However, why the squared? What is so special about it? To non physicists, this question may not seem interesting, but it is extremely suspicious why our nature chooses to square the quantity so that it represents _the_ probability.\n\nThus, in Hopfield Network (and as well as in quantum physics), we have tried to go beyond, and propose alternatives to the rule. In particular we can consider any of these definitions\n\n1. Powers of $n$: $|\\langle x|y_{n}\\rangle|^n$ as described in \\cite{krotov}.\n\n2. Exponentials : $\\exp\\left(\\langle x|y_{n}\\rangle\\right)$ as described in \\cite{Demircigil_2017}.\n\n3. Specially crafted function: $f\\left( \\langle x|y_{n}\\rangle\\right)$ as described in \\cite{hopfieldIsAllYouNeed}.\n\nThe (physical) intuition behind all these is that we are exploring different ways to define the probability of quantum states, the Born's rule. It is often taken as a postulate (cannot be proven) but recent research has shown that it is a direct consequence (the only possibility) of a few even more fundamental requirements of how our world should behave \\cite{Masanes_2019}.\n\nUnfortunately (and interestingly) for quantum physics, there is only one possibility to define probability. However, it seems like the advantages of going beyond classical Hopfield Network are far greater than its classical counterpart \\cite{hopfieldIsAllYouNeed}.\n\n## Final Remark\n\nIn this article, I have illustrated a physical intuition and link between Hopfield Network and Quantum Physics. I should mention that it is, perhaps no surprising that, there is an interesting link between the two, since the former was heavily inspired by physical systems such as spin glasses and Ising model.\n\n\n<br /><br />\n\n<br /><br />\n\n### References\n\n@@bibliography@@\n@@bibliography@@\n"}},{"node":{"frontmatter":{"date":"2020-12-21","description":"Explaining the key differences between RAM in classical and quantum computers, and how to take advantage of it.","title":"The Difference Between Quantum RAM and Classical RAM"},"slug":"posts/quantum-ram","rawBody":"---\ntitle: The Difference Between Quantum RAM and Classical RAM\ndate: \"2020-12-21\"\ndescription: \"Explaining the key differences between RAM in classical and quantum computers, and how to take advantage of it.\"\ncategories: [\"quantum-computing\"]\ntags: [\"quantum computing\"]\n---\n\nimport Image from \"../../src/components/Utils/image\"\n\nRAM, random access memory, is an integral part of any modern computers. We have taken for granted how important RAM is in our daily lives and how easy it is to upgrade our RAM (of course cost is another factor).\n\nA data scientist, data engineer, or quant will surely know that RAM is the first bottleneck in their work: If you cannot load all your data in, you simply have to invest extra time to streamline your analysis.\n\nIn the quantum world (quantum computers to be precise), 'RAM' is also an important tool and some of the quantum algorithms, which supposedly outperform best classical algorithm known, require the availability of the so called quantum RAM, **qRAM** in short. However, qRAM is very different when it comes to its functionalities and implementations.\n\nIn this article, we will first review how a classical RAM works. Then we will describe what are the additional requirements of qRAM. Finally we illustrate the difficulties in its implementations.\n\n## Classical RAM\n\nTo understand how classical RAM works, it is instructive to consider the following cat example:\n\nSuppose you have 8 cute cats, and you keep a note of which cat has been fed, and which is still hungry (poor cat). We shall use the notation as followed: $0$ indicates that the cat is hungry and $1$ indicates that the cat has been fed. For example, in the Figure 1 below, we can see which cats are have been fed (Cat 1/3/4/5/7) and which cats are hungry (Cat 2/6/8).\n\n<Image\n\tfileName=\"quantum-ram/figure-1-cat-data.png\"\n\tclassName=\"medium-image\"\n\talt=\"Figure 1: Cats with indicators whether they have been fed already or still hungry.\"\n/>\n\nThey are stored in RAM as binary data: RAM will assign an address to the location it stores the data\n\n| Cats Index | Status | RAM Data | RAM Address |\n| :--------: | :----: | :------: | :---------: |\n|   Cat 1    |  Full  |    1     |    0x000    |\n|   Cat 2    | Hungry |    0     |    0x001    |\n|   Cat 3    |  Full  |    1     |    0x010    |\n|   Cat 4    |  Full  |    1     |    0x011    |\n|   Cat 5    |  Full  |    1     |    0x100    |\n|   Cat 6    | Hungry |    0     |    0x101    |\n|   Cat 7    |  Full  |    1     |    0x110    |\n|   Cat 8    | Hungry |    0     |    0x111    |\n\nNotice that I have also included the address pointing to the location where the RAM data of each cat is being stored.\n\n## Classical RAM Retrieval\n\nIn order to retrieve the data, computer will let the RAM knows the address we are interested, and the RAM will retrieve the data. The way this works in practise is to use the _fanout_ algorithm \\cite{circuitDesign}.\n\nSuppose we are interested to get the status of Cat 7, which corresponds to the address $0x110$. In this case the only relevant information is the last 3 digits of the address, $110$.\n\nAs shown in Figure 2 below, the first binary digit will control the gate in the first level and it will allow transmission only to the right. The second digits will control _all_ the gates corresponding to the second level. In this case, all the Level 2 gates will also allow transmission only to the right. The third binary in the address will control the Level 3 gates and in this case, allowing only transmission to the left.\n\n<Image\n\tfileName=\"quantum-ram/figure-2-fanout-algorithm.png\"\n\tclassName=\"medium-image\"\n\talt=\"Figure 2: Classical RAM fanout algorithm.\"\n/>\n\nOnce the gates are configured correctly, it is then easy to retrieve the data from the corresponding address. Note that there are different implementations but _fanout_ algorithm is the most common implementaton.\n\n## Problems Extending to Quantum Systems\n\nThe immediate question is how can we apply the same implementation in quantum computers. The short answer is it is difficult. Here are the reasons.\n\n1. **Superposition** - In the classical RAM, there is always a unique address that we are interested in. Sure, we may want to find out what are the data stored in address 1, address 2, ... and so on, but the we will never _mix_ up query address 1 and address 2 together. In quantum computers, it is necessary to be able query a **superposition** of the quantum state in address 1 with the quantum state of address 2. For instance, we want the state $ \\frac{1}{\\sqrt{2}} \\left( |0x001\\rangle + |0x110\\rangle  \\right) $. There is no classical analogue to this.\n\n2. **Decoherence** - Classical RAM can in principle store the data as long as its power is on. Quantum computers do not have such luxury because quantum systems are extremely delicate. Quantum memory will decohere (destroyed into unusable state) in a short time span. How short is the time scale we are talking? Miliseconds \\cite{Miao1493}.\n\n3. **Entanglement** - Entanglement is a double edged sword. It is a useful feature but it is also a pain to deal with. In the Figure 2 above, you may notice that the second bit of the address controls _all_ the gates at the second level. The number of gates at the $n^{th}$ level scales exponentially. In a quantum computer, this would mean that we need to interact exponentially many quantum objects and such interaction will create a massive entanglement between all the objects. This kind of system is extremely prone to decoherence.\n\n## Quantum RAM\n\nDespite the problems addressed above, there are advances in terms of algorithms and implementations. Here I illustrate the algorithm called **bucket-brigade** quantum RAM \\cite{qramPRL,qramPRA}.\n\nTo simplify the notations, I shall reduce the levels to only two levels. Thus the addresses can only be one of the following ${0x00,0x01,0x10,0x11}$. Inside the quantum RAM, holds the information, which in this quantum states. The following table provides a summary.\n\n|              Address              |                 States                 |\n| :-------------------------------: | :------------------------------------: |\n| $\\left\\vert0x00\\right\\rangle_{A}$ | $\\left\\vert Cat_{00}\\right\\rangle_{M}$ |\n| $\\left\\vert0x01\\right\\rangle_{A}$ | $\\left\\vert Cat_{01}\\right\\rangle_{M}$ |\n| $\\left\\vert0x10\\right\\rangle_{A}$ | $\\left\\vert Cat_{10}\\right\\rangle_{M}$ |\n| $\\left\\vert0x11\\right\\rangle_{A}$ | $\\left\\vert Cat_{11}\\right\\rangle_{M}$ |\n\nThe subscripts $A$ and $M$ here are to distinguish the systems from address vs memory states.\n\nHowever, remember that we have a quantum computer: thus, any _linear combination_ of the addresses are also valid. For example, a user can specify the following valid address:\n\n$$\n\\alpha_{00} \\left|0x00\\right\\rangle_{A} + \\alpha_{11} \\left|0x11\\right\\rangle_{A}\n$$\n\nwhere $\\left|\\alpha_{00}\\right|^2 + |\\alpha_{11}\\vert^2 = 1$. The quantum computer, together with quantum RAM, should return the user the following states:\n\n$$\n\\alpha_{00}\n\\left|0x00\\right\\rangle_{A}\n\\left|Cat_{00}\\right\\rangle_{M}\n+\n\\alpha_{11}\n\\left|0x11\\right\\rangle_{A}\n\\left|Cat_{11}\n\\right\\rangle_{M}\n$$\n\n## Quantum RAM Retrieval Process\n\nTo understand the idea behind the retrieval from a quantum RAM, the authors proposed that we first prepare our quantum RAM in the following states as shown in the figure below. The circles refer to a switch is initially in the off state.\n\n<Image\n\tfileName=\"quantum-ram/figure-3-BB-step1.png\"\n\tclassName=\"small-image\"\n\talt=\"Figure 3: Initialization of quantum RAM.\"\n/>\n\nWe then sequentially send the quantum address from _top to bottom_, one qubit by one qubit. To understand the process, it is good to start with simple cases.\n\n### A. Classical address in quantum RAM\n\nSuppose we interested to retrieve the quantum state $\\left|Cat_{11}\\right\\rangle_{M}$, we should send in the address $\\left|\n0x{\\color{red}{1}}{\\color{green}{1}}\n\\right\\rangle_{A}$. What happens is that we will send in the address one by one (they are really just qubit) starting from the first qubit (highlighted as <span style=\"color:red\">red</span>) then the second qubit (highlighted as <span style=\"color:green\">green</span>). This is illustrated in Figure 3 below.\n\n<Image\n\tfileName=\"quantum-ram/figure-3-BB-simple.png\"\n\tclassName=\"small-image\"\n\talt=\"Figure 3: Retrieval of classical address using quantum RAM.\"\n/>\n\n1. Step 1: We send in the first qubit to interact with the first switch, since the switch is off state, it will then interact with the first qubit which is in the state $\\color{red}{1}$, and remember that $1$ means transmission to the right.\n2. Step 2: We then send in the second qubit. By this time, the first switch has already interacted with first qubit, it will simply direct the second qubit to the right, and down to the switch on the lower right. Since that switch is in the off state, it will then interact with the second qubit, which is in state $\\color{green}{1}$ and opens up pathway to the right.\n3. Step 3: We then send in an empty state down the pathway. This empty quantum state, $\\left|\\cdot\\right\\rangle_{M}$ will flow down all the way and reaches the memory housing the desired quantum state $\\left|Cat_{11}\\right\\rangle_{M}$. Then the memory state can be coherently transfered to the empty state.\n\n### B. Quantum address in quantum RAM\n\nLet's see what happens when we specify a quantum address $\\alpha_{00} \\left|0x{\\color{red}{0}}0\\right\\rangle_{A} + \\alpha_{11} \\left|0x{\\color{red}{1}}1\\right\\rangle_{A}$.\nLet's send the first qubit (highlighted in red) into the quantum RAM. (Remember that even though there are two terms in the equation, there is really just one qubit encoding the first address, it is in suporposition!)\n\nWhen the first qubit reaches the first switch, the quantum switch is unable to deterministically open up pathway to the right nor to the right because the qubit is not in a deterministic state of either $0$ or $1$. So, the switch will _open up both pathway_. This is the interesting features of quantum physics. However, the switch will open up the pathway to both directions in a way that preserve quantum coherence (that is until decoherence kicks in).\n\n<Image\n\tfileName=\"quantum-ram/figure-4-BB-quantum-first-qubit.png\"\n\tclassName=\"small-image\"\n\talt=\"Figure 3: Retrieval of quantum address using quantum RAM. Only the first qubit is sent in.\"\n/>\n\nNow we send in the second qubit. Which way will the second qubit travel? The answer is both. Otherwise, ask for refund. In the figure below, we use transparent arrows to show that qubit branch out into both directions but in a coherent manner. The wave function of the second qubit that reaches the switch at the bottom left, will interact with it but this time, it only open up the pathway to the left, which is the memory state $\\left|Cat_{00}\\right\\rangle_{M}$. This is because we do not have any weight to the address $\\left|0x01\\right\\rangle_{A}$.\n\nOn the other hand (or in a parallel universe), the wave function of the qubit that reaches the switch at the bottom right will interact with the switch and opens up the pathway only to the right.\n\n<Image\n\tfileName=\"quantum-ram/figure-5-BB-quantum-second-qubit.png\"\n\tclassName=\"small-image\"\n\talt=\"Figure 3: Retrieval of quantum address using quantum RAM. After second qubit is sent in.\"\n/>\n\nFinally, we send in the empty qubit through the pathway and it will reach both the $\\left|Cat_{00}\\right\\rangle_{M}$ and $\\left|Cat_{11}\\right\\rangle_{M}$. The empty qubit will then capture the memory quantum states and travel back to the starting point. The end result will be exactly what we wanted.\n\n$$\n\\alpha_{00}\n\\left|0x00\\right\\rangle_{A}\n\\left|Cat_{00}\\right\\rangle_{M}\n+\n\\alpha_{11}\n\\left|0x11\n\\right\\rangle_{A}\n\\left|Cat_{11}\n\\right\\rangle_{M}\n$$\n\n## Final Remark\n\nI hope you have gained a basic understanding of the problems and difficulties in implementing quantum RAM in a quantum computer. The ability to prepare and retrieve quantum states using the above qRAM algotirhm is essential for many algorithms \\cite{Rebentrost_2018}.\n\nEven though we are still some distance away from quantum computer, I do look forward to the day data scientists, data engineer and quants and program and code in the language of quantum computing.\n\n\n<br /><br />\n\n<br /><br />\n\n### References\n\n@@bibliography@@\n@@bibliography@@\n"}}]}}}